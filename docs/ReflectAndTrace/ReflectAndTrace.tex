\documentclass{article}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{longtable}
\usepackage{hyperref}

\title{Reflection and Traceability Report on \progname}

\author{\authname}

\date{}

\input{../Comments}
\input{../Common}

\begin{document}

\maketitle

% \plt{Reflection is an important component of getting the full benefits from a
% learning experience. Besides the intrinsic benefits of reflection, this
% document will be used to help the TAs grade how well your team responded to
% feedback. Therefore, traceability between Revision 0 and Revision 1 is an
% important part of the reflection exercise. In addition, several CEAB (Canadian
% Engineering Accreditation Board) Learning Outcomes (LOs) will be assessed based
% on your reflections.}

\section{Changes in Response to Feedback}
This section summarizes the changes made over the course of the capstone project in response to
feedback from sources such as TAs, the supervisor and other teams. The associated commits can be
found by clicking on the associated issue created.

% \plt{Summarize the changes made over the course of the project in response to
% feedback from TAs, the instructor, teammates, other teams, the project
% supervisor (if present), and from user testers.}

% \plt{For those teams with an external supervisor, please highlight how the feedback 
% from the supervisor shaped your project.  In particular, you should highlight the 
% supervisor's response to your Rev 0 demonstration to them.}

% \plt{Version control can make the summary relatively easy, if you used issues
% and meaningful commits.  If you feedback is in an issue, and you responded in
% the issue tracker, you can point to the issue as part of explaining your
% changes.  If addressing the issue required changes to code or documentation, you
% can point to the specific commit that made the changes.  Although the links are
% helpful for the details, you should include a label for each item of feedback so
% that the reader has an idea of what each item is about without the need to click
% on everything to find out.}

% \plt{If you were not organized with your commits, traceability between feedback
% and commits will not be feasible to capture after the fact.  You will instead
% need to spend time writing down a summary of the changes made in response to
% each item of feedback.}

% \plt{You should address EVERY item of feedback.  A table or itemized list is
% recommended.  You should record every item of feedback, along with the source of
% that feedback and the change you made in response to that feedback.  The
% response can be a change to your documentation, code, or development process.
% The response can also be the reason why no changes were made in response to the
% feedback.  To make this information manageable, you will record the feedback and
% response separately for each deliverable in the sections that follow.}

% \plt{If the feedback is general or incomplete, the TA (or instructor) will not
% be able to grade your response to feedback.  In that case your grade on this
% document, and likely the Revision 1 versions of the other documents will be
% low.} 

\subsection{SRS and Hazard Analysis}
Here is the feedback we received on the SRS and Hazard Analysis documents, and the changes we made in response to that feedback.

\begin{longtable}{| p{0.2\textwidth} | p{0.2\textwidth} | p{0.3\textwidth} | p{0.1\textwidth} |}
    \caption{Feedback and Changes for SRS Documentation} \\
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endfirsthead
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endhead
    \hline
    \endfoot
    TA Feedback & Formalization & \textbf{Did Not Fix}: Already formalized to the best of our ability & \href{https://github.com/TPGEngine/tpg/issues/311}{\#311}\\
    \hline
    TA Feedback & Extension of Knowledge & Mentioned and cited sources where terms are taken from. & \href{https://github.com/TPGEngine/tpg/issues/310}{\#310} \\
    \hline
    TA Feedback & Verifiable Requirements & Updated requirements to ensure they were testable and measurable. & \href{https://github.com/TPGEngine/tpg/issues/309}{\#309}\\
    \hline
    TA Feedback & Traceable Requirements & Added traceability matrix to enhance traceability. & \href{https://github.com/TPGEngine/tpg/issues/308}{\#308} \\
    \hline
    TA Feedback & What not How (Abstract) & Revised some requirements to focus on "what" the system should do rather than "how" it should do it. & \href{https://github.com/TPGEngine/tpg/issues/307}{\#307} \\
    \hline
    TA Feedback & Content of SRS (Functionality and Specificity) & Revised functional requirements and clarified ambiguous sections. & \href{https://github.com/TPGEngine/tpg/issues/305}{\#305} \\
    \hline
    TA Feedback & Formatting and Style & Modified formatting according to feedback. & \href{https://github.com/TPGEngine/tpg/issues/306}{\#306} \\
    \hline
    Peer Review & Project Goals & \textbf{Did Not Fix}: Decided to focus on main goals of project. & \href{https://github.com/TPGEngine/tpg/issues/106}{\#106} \\
    \hline
    Peer Review & Verifiability & Adjusted specified requirements for verifiability.  & \href{https://github.com/TPGEngine/tpg/issues/105}{\#105} \\
    \hline
    Peer Review & User Business & Clarified problem context. & \href{https://github.com/TPGEngine/tpg/issues/104}{\#104} \\
    \hline
    Peer Review & Dev Planning & Updated development planning section with expected deadlines. & \href{https://github.com/TPGEngine/tpg/issues/102}{\#102} \\
    \hline
    Peer Review & Data Dictionary and Scope & \textbf{Did Not Fix} & \href{https://github.com/TPGEngine/tpg/issues/101}{\#101} \\
    \hline
    Peer Review & Maintainability, Supportability, Adaptability Requirements & Adjusted requirements for maintainability, supportability, and adaptability. & \href{https://github.com/TPGEngine/tpg/issues/107}{\#107} \\
    \hline
    Peer Review & Fix Functional Requirements & Revised concerned FR-6 for specificity. & \href{https://github.com/TPGEngine/tpg/issues/103}{\#103} \\
    \hline
\end{longtable}

\begin{longtable}{| p{0.2\textwidth} | p{0.2\textwidth} | p{0.3\textwidth} | p{0.1\textwidth} |}
    \caption{Feedback and Changes for Hazard Analysis} \\
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endfirsthead
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endhead
    \hline
    \endfoot
    TA Feedback & Recommended Actions & \textbf{Did Not Fix}: Addressed in Peer Feedback  & \href{https://github.com/TPGEngine/tpg/issues/314}{\#314} \\
    \hline
    TA Feedback & Hazard Identification & Adjusted concerned sections with feedback. & \href{https://github.com/TPGEngine/tpg/issues/313}{\#313} \\
    \hline
    TA Feedback & Spelling and Grammar & Corrected spelling and grammar errors and implemented other feedback specified. & \href{https://github.com/TPGEngine/tpg/issues/312}{\#312} \\
    \hline
    Peer Review & Inconsistent Hazard Reference & Fixed inconsistency between hazard references. & \href{https://github.com/TPGEngine/tpg/issues/136}{\#136} \\
    \hline
    Peer Review & Potential Missing Hazard for FMEA & \textbf{Did Not Fix}: Having these experiments that produce poor results could be a good thing that lets us tweak the TPG algorithm  & \href{https://github.com/TPGEngine/tpg/issues/135}{\#135} \\
    \hline
    Peer Review & Priority Assignment & \textbf{Did Not Fix}: Not required & \href{https://github.com/TPGEngine/tpg/issues/133}{\#133} \\
    \hline
    Peer Review & No Mitigation Strategy & Modify mitigation strategies for hazards. & \href{https://github.com/TPGEngine/tpg/issues/132}{\#132} \\
    \hline
    Peer Review & Prioritization Justification & Provided detailed justification for hazard prioritization. & \href{https://github.com/TPGEngine/tpg/issues/130}{\#130} \\
    \hline
    Peer Review & SRS Linking Roadmap & Linked SRS in roadmap to hazard analysis. & \href{https://github.com/TPGEngine/tpg/issues/128}{\#128} \\
    \hline
    Peer Review & Ambiguous Terms & Clarified ambiguous terms in the hazard analysis. & \href{https://github.com/TPGEngine/tpg/issues/134}{\#134} \\
    \hline
\end{longtable}

\subsection{Design and Design Documentation}
Here is the feedback we received on the design documents (MG and MIS), and the changes we made in response to that feedback.
\begin{longtable}{| p{0.2\textwidth} | p{0.2\textwidth} | p{0.3\textwidth} | p{0.1\textwidth} |}
    \caption{Feedback and Changes for Module Guide} \\
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endfirsthead
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endhead
    \hline
    \endfoot
    TA Feedback & Quality Information & Fixed all addressed concerns with issue. & \href{https://github.com/TPGEngine/tpg/issues/346}{\#346} \\
    \hline
    Peer Review & Lack of Links to Other Documents & Added links and references to related documents/sections for better traceability. & \href{https://github.com/TPGEngine/tpg/issues/242}{\#242} \\
    \hline
    Peer Review & Module Decomposition & \textbf{Did Not Fix}: Decomposition was deemed unnecessary for the current scope. & \href{https://github.com/TPGEngine/tpg/issues/240}{\#240} \\
    \hline
\end{longtable}

\begin{longtable}{| p{0.2\textwidth} | p{0.2\textwidth} | p{0.3\textwidth} | p{0.1\textwidth} |}
    \caption{Feedback and Changes for Module Specification Interface} \\
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endfirsthead
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endhead
    \hline
    \endfoot
    TA Feedback & Sketches for Enough to Build & \textbf{Did Not Fix}: Did not include additional sketchecs or examples, as current level of detail seemed sufficient for our project scope. & \href{https://github.com/TPGEngine/tpg/issues/347}{\#347} \\
    \hline
    Peer Review & Confusion on TPG Experiment Module & Clarified confusing sections in the module specification interface. & \href{https://github.com/TPGEngine/tpg/issues/245}{\#245} \\
    \hline
    Peer Review & Lack of Info for Independent Developer & Added additional details to support independent developers. & \href{https://github.com/TPGEngine/tpg/issues/243}{\#243} \\
    \hline
    Peer Review & Incorrect "Uses" in MIS & Corrected "Uses" subsections in the module specification interface for modules. & \href{https://github.com/TPGEngine/tpg/issues/244}{\#244} \\
    \hline
\end{longtable}
\subsection{VnV Plan and Report}

Here is the feedback we received on the VnV Plan and VnV Report, and the changes we made in response to that feedback.
\begin{longtable}{| p{0.2\textwidth} | p{0.2\textwidth} | p{0.3\textwidth} | p{0.1\textwidth} |}
    \caption{Feedback and Changes for VnV Plan} \\
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endfirsthead
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endhead
    \hline
    \endfoot
    TA Feedback & Inaccurate descriptions in 2.1 Summary and 2.2 Objective  & Modified the two sections to better reflect rubric & \href{https://github.com/TPGEngine/tpg/issues/315}{\#315} \\
    \hline
    TA Feedback & Improve Spelling and Grammar and Style of VnV Plan  & Addressed feedback given in issue & \href{https://github.com/TPGEngine/tpg/issues/316}{\#316} \\
    \hline
    Peer Feedback & Add traceability in VnV Plan & \textbf{Did Not Fix}: Already have proper traceability & \href{https://github.com/TPGEngine/tpg/issues/150}{\#150} \\
    \hline
    Peer Feedback & Increase Detail in Nonfunctional Test Descriptions & Addressed feedback in issue by adding details & \href{https://github.com/TPGEngine/tpg/issues/151}{\#151} \\
    \hline
     Peer Feedback & Survey Questions for Documentation & Added survey questions & \href{https://github.com/TPGEngine/tpg/issues/155}{\#155} \\
    \hline


\end{longtable}



\begin{longtable}{| p{0.2\textwidth} | p{0.2\textwidth} | p{0.3\textwidth} | p{0.1\textwidth} |}
    \caption{Feedback and Changes for VnV Report} \\
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endfirsthead
    \hline
    \textbf{Feedback Source} & \textbf{Feedback Item} & \textbf{Response} & \textbf{Issue} \\
    \hline
    \endhead
    \hline
    \endfoot
    Peer Feedback & Limited Explanation of Changes Due to Testing & \textbf{Did Not Fix}: We did address the VS Code Dev Containers as a response & \href{https://github.com/TPGEngine/tpg/issues/396}{\#396} \\
    \hline
    Peer Feedback & Limited Explanation of Changes Due to Testing   & \textbf{Did Not Fix}: Deemed not relevant for VnV Report.  & \href{https://github.com/TPGEngine/tpg/issues/398}{\#398} \\
    \hline
    Peer Feedback & Missing Test Description for FR-SNL6 in both VnV documents  & Updated the VnVReport with the test's full description in the same format as other tests & \href{https://github.com/TPGEngine/tpg/issues/398}{\#398} \\
    \hline
    Peer Feedback & Traceability Matrix Inconsistency  & Removed test to get rid of inconsistency & \href{https://github.com/TPGEngine/tpg/issues/398}{\#398} \\
    \hline
    Peer Feedback & Lack of Criterion for Usability Tests & \textbf{Did Not Fix}: already added justifications on what constitutes a pass through the scores we assigned users  & \href{https://github.com/TPGEngine/tpg/issues/400}{\#400} \\
    \hline
    Peer Feedback & Missing NFR test case that is present in the VnVPlan & Included proper NFR solution & \href{https://github.com/TPGEngine/tpg/issues/401}{\#401} \\
    \hline
    Peer Feedback & Lack of clarity with feedback changes & \textbf{Did Not Fix}: Already talked about the improvements & \href{https://github.com/TPGEngine/tpg/issues/402}{\#402} \\
    \hline

\end{longtable}

\section{Challenge Level and Extras}

\subsection{Challenge Level}
The challenge level of the project is \textbf{advanced} as agreed upon by the course instructor since this project is an extension of the current Tangled Program Graphs repository created by Dr. Stephen Kelly.

\subsection{Extras}
The extra tackled by this project is a Research Report. This report will cover the research and results obtained from incorporating dynamic memory to enhance reinforcement learning within MuJoCo using Tangled Program Graphs.
It explores this through single-task (STL) and multi-task (MTL) experiments on MuJoCo environments such as Inverted Pendulum, Half Cheetah, and Humanoid Standup.
The Research Report can be found \href{https://github.com/TPGEngine/tpg/blob/main/docs/Extras/ResearchReport(Advanced)/ResearchReport.pdf}{here}.

\section{Design Iteration (LO11 (PrototypeIterate))}

The design and implementation of our capstone project were driven by the needs of our client, Dr. Kelly, and his research group. As contributors to their existing \href{https://gitlab.cas.mcmaster.ca/kellys32/tpg}{codebase}, our efforts focused on addressing pain points in current workflows and enhancing the overall developer experience, aligning with the goal of facilitating research on the TPG framework within more complex environments.

\subsection{MuJoCo Environment Integration and Expansion}

Our initial work involved integrating MuJoCo environments to expand the capabilities of the TPG framework. Dr. Kelly's group had previously implemented the "Ant" environment. Recognizing the need to evaluate the TPG framework in more advanced physics engines, we implemented support for additional environments such as Inverted Pendulum, Humanoid Standup, and Half Cheetah. This directly addressed Dr. Kelly's desire to transition to MuJoCo, enabling experimentation within more sophisticated and realistic simulations.

This effort culminated in addressing Dr. Kelly's primary research question: evaluating TPG's performance in multi-task scenarios. Building upon the individual environment implementations, we iterated on combining MuJoCo environments, enabling the evolution of policies capable of learning across multiple tasks. This progression demonstrates a clear evolution driven by the client's research objectives, moving from basic integration to tackling more complex, multi-task learning problems.

\subsection{Migration to .csv Logging}

A significant change involved modifying the logging system. Dr. Kelly expressed a desire to transition data analysis processing from R to Python-based tools, citing maintainability and the flexibility of Python libraries as key motivations. To accommodate this transition, we migrated the logging format from \texttt{.std} and \texttt{.err} files to \texttt{.csv} files.

The choice of \texttt{.csv} over the previous \texttt{.std} format offers several advantages from the perspective of user needs:

\begin{itemize}
    \item \textbf{Organization:} \texttt{.csv} files provide a structured, tabular format. This makes it easier to import and analyze data using Python libraries like Pandas, streamlining the data processing pipeline. The previous \texttt{.std} format required more complex parsing and was less amenable to automated analysis.
    \item \textbf{Accessibility:} \texttt{.csv} is a widely supported and easily accessible format. Researchers can readily open and manipulate \texttt{.csv} files in various software packages, fostering collaboration and simplifying data sharing.
    \item \textbf{Specific Metric Capture:} We designed the \texttt{.csv} logs to capture specific metrics from each stage of the evolutionary process. This allows for targeted analysis of performance and behavior, providing Dr. Kelly and his team with the data they need to evaluate the TPG framework effectively.
\end{itemize}

By migrating to \texttt{.csv} logs, we not only accommodated Dr. Kelly's preference for Python-based tools but also improved the organization, accessibility, and analytical potential of the logged data, directly benefiting the research workflow.

\subsection{CLI Tool Development}

Initially, the execution of experiments relied on script-based execution, requiring developers to navigate to specific directories and execute commands with specific parameters. This workflow was identified as a pain point, particularly when running multiple experiments concurrently.

In response to this usability issue, we developed a Command Line Interface (CLI) tool to simplify experiment execution. This change was presented to Dr. Kelly, who agreed that a streamlined workflow would significantly improve the developer experience.

The initial "MVP" version of the CLI tool supported essential functions such as:

\begin{itemize}
    \item \texttt{tpg evolve [environment]}: Evolving a policy for a given environment.
    \item \texttt{tpg replay [environment]}: Replaying the best-performing agent in an environment.
    \item \texttt{tpg plot [environment]}: Plotting relevant statistics.
\end{itemize}

These commands mirrored the functionality of the original scripts but offered a more intuitive and descriptive interface. After merging the initial version and providing documentation, Dr. Kelly's research group provided valuable feedback. They requested additional functionality, such as:

\begin{itemize}
    \item \texttt{tpg clean}: Removing old log files to maintain a clean working environment.
    \item \texttt{tpg kill}: Terminating MPI processes.
\end{itemize}

Based on this feedback, we iterated on the CLI tool, incorporating these new features to further streamline the user experience and address the specific needs of the research team. The final CLI tool represents a significant improvement in usability, allowing researchers to execute, manage, and analyze experiments with greater ease and efficiency.

In summary, our design iterations were continuously guided by the needs of Dr. Kelly and his research group. By focusing on improving existing workflows, accommodating new technologies, and responding to user feedback, we were able to develop a system that directly supports their research goals and enhances the overall developer experience.


\section{Design Decisions (LO12)}

\plt{Reflect and justify your design decisions.  How did limitations,
 assumptions, and constraints influence your decisions?  Discuss each of these
 separately.}

\section{Economic Considerations (LO23)}

Since the project will eventually be open-sourced once the research work is finished, its primary market does
not include direct commercial sales but rather the research community, AI practitioners, and robotics engineers 
and enthusiasts. 

Attracting users will require a combination of strategies to enhance community engagement,
increase project visibility, and improve usability. A few key approaches are:

\begin{itemize}
    \item \textbf{Open-Source Repository}: Hosting the project on GitHub with a well-organized README, issue tracker and contribution
        guidelines would encourage potential contributors.
    \item \textbf{Conference \& Workshop Presentations}: Presenting the project at ML and robotic conferences such as GECCO and Conference
        on Artificial Life would increase its visibility among researchers.
    \item \textbf{Publishing Research \& Documentation}: Continue releasing research papers and technical reports explaining TPG's 
        capabalities and how well it compares with alternatives.
    \item \textbf{Showcasing Notable Results}: Publishing blog posts, YouTube videos, as well as engaging with the reinforcement learning community
        through Hugging Face forums, GitHub discussions, Reddit and other online platforms. 
\end{itemize}

TPG's potential lies in academic research, robotics applications, and open-source adoption. The long-term strategy 
would focus on gaining traction in research and industry collaborations, which could lead to funding opportunities, 
grants, or potential commercial applications in reinforcement learning for real-world robotic systems.

While it's difficult to estimate exact numbers of potential users, open-source RL-based GitHub projects often have
thousands of users. For example, OpenAI's Gym has 35.6k stars GitHub, while Stable Baselines3 has around 10.2k stars. Given 
TPG's unique focus, it has the potential to attract a niche but highly engaged user base of researchers, engineers, 
and developers in the AI and robotics space.

\section{Reflection on Project Management (LO24)}

\plt{This question focuses on processes and tools used for project management.}

\subsection{How Does Your Project Management Compare to Your Development Plan}

\plt{Did you follow your Development plan, with respect to the team meeting plan, 
team communication plan, team member roles and workflow plan.  Did you use the 
technology you planned on using?}

Our team properly followed our development plan with respect to the workflow plan, team member roles, and communication plan. As stated in our Development Plan, we used the "TPG Capstone" Discord server for team communication, which was consistently utilized throughout the project to discuss ideas, host meetings and notify the team for PR requests. Additionally, the development stack and technologies mentioned in the Development Plan, such as Git, GitHub, and C++, were integral to the development of our project.

We made sure that each team member adhered to their designated roles and responsibilities by assigning each member tasks that matched their roles. Our team meetings were held regularly, and we successfully coordinated schedules to accommodate everyone, leading to regular team attendance. Overall, our project management was carried out efficiently, staying true to our original plan.


\subsection{What Went Well?}

\plt{What went well for your project management in terms of processes and 
technology?}

Our project management was effective, particularly in the implementation of MuJoCo environments, which proceeded smoothly. This success allowed us to build upon our initial work by exploring multi-tasking for the TPG-Mujoco interface. Additionally, we enhanced our project with the development of new CLI tools, which streamlined our workflow and improved the overall efficiency of our processes. Our team communication and collaboration were strong throughout the project. This ensured that all team members were aligned with project goals and timelines. The use of GitHub for version control and project management also contributed to our success, allowing us to track progress effectively and manage tasks efficiently through issues. Overall, these factors combined to create a productive working environment.
\subsection{What Went Wrong?}

\plt{What went wrong in terms of processes and technology?}

One of the challenges we faced was the inability to implement all the MuJoCo environments we initially planned due to unforeseen complications. Additionally, the process of migrating from SCons to CMake proved to be quite challenging due to a lack of updated documentation. Debugging C++ was particularly difficult as changes were occurring regularly in the TPG framework, and our limited experience with C++ added to the complexity. These issues highlighted areas where we needed to improve our skills and adapt our strategies to better handle such challenges in the future.

\subsection{What Would you Do Differently Next Time?}

\plt{What will you do differently for your next project?}

In future projects, we would aim to diversify our roles within the team, allowing each member to explore multiple areas of software development. This would encourage a more varied approach and broaden our collective skill set, as it challenges us to step outside our comfort zones and tackle different aspects of the development process. Additionally, experimenting with a different development technologies and frameworks could be both exciting and beneficial, as it presents new challenges and learning opportunities. While our current stack was effective, exploring newer frameworks or languages might enhance performance and developer efficiency.

We would also likely place a greater emphasis on more comprehensive unit testing to ensure robust code quality, as our testing scope was relatively limited. This would involve more inclusive testing strategies that cover a wider range of scenarios and edge cases. We would likely also have implemented more complex MuJoCo environments for evaluation, which would have provided a deeper understanding of the TPG framework's capabilities and limitations in handling sophisticated simulations.

\section{Reflection on Capstone}

This section focuses on the key learnings and reflections gained during the course of the capstone project.

\subsection{Which Courses Were Relevant}

Several courses proved highly relevant to the successful completion of our capstone project. Among the most impactful were:

\begin{enumerate}
    \item \textbf{2AA4 - Introduction to Software Design:} This course provided a foundational understanding of collaborative software development practices. The experience of working on a group project using GitHub, including managing issues, utilizing Kanban boards, and creating pull requests, was invaluable. It instilled the fundamentals of effective team-based development. Furthermore, the emphasis on software design principles, such as SOLID and GRASP, was directly applicable to the capstone project. Although the course was primarily focused on Java, the object-oriented design patterns learned were transferable to our project, which utilized Python and C++. Specifically, we leveraged the Observer pattern to implement a refined logging system within the TPG codebase.

    \item \textbf{3RA3 - Requirements Engineering:} This course provided a solid foundation in defining project requirements prior to implementation. Learning to articulate and prioritize user needs was instrumental in guiding our development efforts. The user-based design principles learned in 3RA3 helped us focus our efforts and prioritize features that aligned with Dr. Kelly's research objectives. The reports we submitted throughout the capstone project mirrored the structure and content of assignments completed in 3RA3, demonstrating the practical application of the course's principles.
\end{enumerate}

\subsection{Knowledge/Skills Outside of Courses}

Beyond the formal curriculum, our capstone project required the acquisition of specific knowledge and skills in areas not explicitly covered in our coursework. These included:

\begin{enumerate}
    \item \textbf{Genetic Programming:} Gaining an understanding of genetic programming, a specialized research area, was essential. Since none of us had prior exposure to this domain, our learning primarily involved direct instruction from our supervisor, Dr. Kelly, as well as in-depth analysis of the existing TPG codebase. This process required us to quickly grasp complex concepts and apply them to our project.

    \item \textbf{C++ Proficiency:} C++ was the language of choice for performance-critical aspects of the project, a decision driven by Dr. Kelly's preference. While our curriculum included object-oriented languages like Java, C++ was not a primary focus. We therefore had to develop proficiency in C++ independently, leveraging our understanding of object-oriented principles to navigate the language's syntax and intricacies.

    \item \textbf{Docker Containerization:} The project made extensive use of Docker for containerization, a topic not typically covered in our academic coursework. While some of us had gained exposure to Docker during co-op placements, a deeper understanding of how containers work and their benefits was necessary. We independently learned how to install, run, and utilize Docker, developing a valuable skill essential in modern software development workflows. Given the prevalence of containerization in industry, incorporating some introductory material on this topic into the curriculum could benefit future students.
\end{enumerate}


\end{document}