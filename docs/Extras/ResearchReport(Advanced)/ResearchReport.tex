%%
%% This is file `ResearchReport.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from ResearchReport.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro~\cite [option:text,text]
%TC:macro~\citep [option:text,text]
%TC:macro~\citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{afterpage}
\usepackage[abs]{overpic}


\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}
\settopmatter{printfolios=true,printccs=false, printacmref=false}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}

% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}
% %% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[]{Capstone Research Report}{March 2025}{Hamilton, ON, Canada}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% \acmBooktitle{Capstone Research Report, March 2025, Hamilton, ON, Canada}

% \acmISBN{978-1-4503-XXXX-X/2025/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command~\citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\setcopyright{none}
\makeatletter
\renewcommand\@formatdoi[1]{\ignorespaces}
\makeatother
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Dynamic Memory in Tangled Program Graphs]{Enhancing Reinforcement Learning for MuJoCo Tasks with Dynamic Memory in Tangled Program Graphs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Cyruss Allen Amante}
\affiliation{%
  \institution{McMaster University}
  \city{Hamilton}
  \state{Ontario}
  \country{Canada}}
\email{amantec@mcmaster.ca}

\author{Mark Angelo Cruz}
\affiliation{%
  \institution{McMaster University}
  \city{Hamilton}
  \state{Ontario}
  \country{Canada}}
\email{cruzm9@mcmaster.ca}

\author{Edward Gao}
\affiliation{%
  \institution{McMaster University}
  \city{Hamilton}
  \state{Ontario}
  \country{Canada}}
\email{gaoe2@mcmaster.ca}

\author{Richard Li}
\affiliation{%
  \institution{McMaster University}
  \city{Hamilton}
  \state{Ontario}
  \country{Canada}}
\email{li1502@mcmaster.ca}

\author{Calvyn Siong}
\affiliation{%
  \institution{McMaster University}
  \city{Hamilton}
  \state{Ontario}
  \country{Canada}}
\email{siongc1@mcmaster.ca}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  This paper investigates the impact of dynamic memory allocation within 
  Tangled Program Graphs (TPG) for reinforcement learning in continuous control 
  tasks. TPG, an RL framework based on genetic programming, evolves agents composed of interconnected programs. 
  We hypothesize that dynamic memory, which allows agents to adaptively adjust 
  memory representation based on task demands, can enhance learning performance 
  and efficiency compared to fixed-memory approaches. We explore this through 
  single-task (STL) and multi-task (MTL) experiments on MuJoCo environments such as Inverted 
  Pendulum, Half Cheetah, and Humanoid Standup. Our results demonstrate that dynamic 
  memory leads to improved fitness scores and more effective program instruction utilization, 
  particularly in multi-task scenarios, suggesting enhanced adaptability and knowledge sharing. 
  We analyze the trade-offs between learning performance and computational efficiency, 
  providing empirical validation for the theoretical benefits of dynamic memory in 
  the genetic programming approaches to RL such as TPG.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Reinforcement Learning, Multi-Task Learning, Dynamic Memory, 
Tangled Program Graphs, Genetic Programming, MuJoCo, Continuous Control}


% \received{16 March 2025}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Reinforcement learning (RL) has emerged as a powerful paradigm
for training autonomous agents to perform complex tasks through 
trial-and-error interaction with an environment. However, real-world applicability 
demands that agents not only achieve high performance but also adapt efficiently 
to varying task demands and environmental dynamics. This paper focuses on 
enhancing the TPG framework, a genetic programming-based RL method, 
for continuous control tasks in the MuJoCo physics simulation environment. 

Traditional deep reinforcement learning (DRL) methods, while powerful, often require substantial 
computational resources and large datasets for effective training~\cite{Mnih07}. 
In contrast, TPG leverages emergent modularity and an adaptive, dynamically evolving 
program structure, offering a more computationally efficient alternative~\cite{Kelly21}. 

Our key contribution lies in integrating dynamic memory allocation within TPG. We hypothesize 
that allowing TPG agents to adaptively adjust their memory representation based on task demands 
will improve both learning performance and sample efficiency in complex MuJoCo environments. 
We investigate this hypothesis through a series of single-task and multi-task experiments, 
analyzing the trade-offs between performance, computational cost, and effective program structure, 
and details our methodology and experimental setup in Section~\ref{sec:methodology}. We will go on to detail the 
data collection and statistical comparison in Section~\ref{sec:experiments}, and lastly provides discussion and 
conclusions of our findings in Section~\ref{sec:results}.


\subsection{MuJoCo}
A significant domain for RL research, particularly in robotics, is physics 
simulation. MuJoCo (Multi-Joint dynamics with Contact) is a widely used physics 
engine, known for its accurate and efficient simulation of complex dynamics, 
contact forces, and articulated bodies~\cite{Todorov07}. Its ability to simulate realistic 
physics and provide diverse, challenging multi-action continuous control tasks makes MuJoCo an invaluable 
tool for developing and evaluating reinforcement learning algorithms for robotics. 
The unique MTL and Single-Task Learning (STL) environments formulated in this work 
includes the following 6 widely used RL benchmarks found on Gymnasium's 
MuJoCo suite~\cite{Towers07}: Ant, Half Cheetah, Hopper, Humanoid Standup, 
Inverted Pendulum, and Inverted Double Pendulum, Figures ~\ref{fig:mujoco_env}(a) to ~\ref{fig:mujoco_env}(e).

\begin{figure}[t]
  \centering
  \begin{tabular}{cc}
    \includegraphics[width=0.3\linewidth]{assets/ant} &
    \includegraphics[width=0.3\linewidth]{assets/half_cheetah} \\
    (a) Ant & (b) Half Cheetah \\
    \includegraphics[width=0.3\linewidth]{assets/hopper} &
    \includegraphics[width=0.3\linewidth]{assets/humanoid_standup} \\
    (c) Hopper & (d) Humanoid Standup \\
    \includegraphics[width=0.3\linewidth]{assets/inverted_pendulum} &
    \includegraphics[width=0.3\linewidth]{assets/inverted_double_pendulum} \\
    (e) Inverted Pendulum & (f) Inverted Double Pendulum \\
  \end{tabular}
  \caption{MuJoCo Environments used in this work}
  \label{fig:mujoco_env}
  \Description{MuJoCo Environments used in this work}
\end{figure}

\subsection{Dynamic Memory}
Dynamic memory plays a crucial role in evolving program graphs, particularly 
in multi-task learning (MTL), by providing a flexible and adaptive
mechanism for encoding and processing temporal information. Unlike static memory 
architectures, which impose fixed storage structures, dynamic memory allows each 
program within an evolving graph to independently adjust its memory representation 
based on task demands. This adaptability facilitates more efficient learning and 
decision-making, ultimately accelerating evolutionary processes.

Dynamic memory within program graphs consists of two primary types:
\begin{itemize}
  \item \textbf{Vector Memory}: Represents data as structured arrays, allowing operations across multiple related values.
  \item \textbf{Matrix Memory}: Enables higher-dimensional representations, which can encode richer state information 
  and facilitate more complex transformations.
\end{itemize}

Each program selects appropriate memory type based on its computational needs, 
and mutations can modify both the type and dimensionality of the memory structures over the course of evolution.

\subsection{Tangled Program Graphs}
Tangled Program Graphs (TPG) is an RL framework developed by McMaster University’s Creative Algorithm Lab under the guidance of Dr. Stephen Kelly.
It leverages genetic programming principles to evolve agents capable of solving complex tasks in dynamic environments. Traditional RL methods,
such as deep reinforcement learning (DRL), often rely on neural networks that require significant computational resources and large datasets for training.
~\cite{Winkler2025}  In contrast, TPG uses genetic programming to evolve agents that can learn and adapt to their environment through a process of 
selection, mutation, and crossover. This unique approach allows TPG to achieve competitive performance in RL tasks with better computation
efficiency than DRL methods.~\cite{Winkler2025}

TPG revolves around the concept of emergent modularity, where agents are composed of interconnected programs that map environmental states to actions.
These programs are organized into hierarchical structures, known as program graphs, that allow agents to break complex tasks into simpler subtasks.~\cite{Kelly21} To incorporate TPG with multi-task learning, an agent will have to be trained to perform multiple tasks sequentially.
This is challenging as the agent must balance different objectives and environments while  avoiding catastrophic forgetting
(learning a new task causes agent to forget previously learned tasks, effectively wasting progress).~\cite{Kelly21} TPG’s
hierarchical and modular structure however, is able to tackle this challenge by allowing agents to dynamically adapt to different tasks by 
“recombining specialized behaviors”.~\cite{Kelly21} TPG has been previously used to evolve agents capable of solving six distinct RL benchmarks from OpenAI’s
Classic Control suite, including CartPole, Acrobot, and Pendulum.~\cite{Kelly21} Currently, TPG is being further developed to gauge it’s
capability of integrating multi-task learning with more complex MuJoCo environments.

\section{Motivation}
The primary motivation behind this research and development effort is to enhance the performance and efficiency of TPG in complex environments, specifically within the MuJoCo physics simulation framework. MuJoCo environments, such as Ant, Half Cheetah, and Humanoid Standup, present significant challenges due to their high-dimensional state and action spaces, as well as their dynamic nature. These environments require agents to learn proper control policies that generalize across diverse scenarios, making them ideal benchmarks for evaluating the scalability and adaptability of TPG. Unlike the previous environments that were explored with TPG such as the Atari video game testbed or OpenAI’s Classic Control suite, MuJoCo environments are much more similar to real-world, complex, and dynamic robotics problems. This makes them a critical step forward in testing TPG’s applicability to real-world applications, such as autonomous robotics and embedded systems.

The goal is to both improve performance and increase efficiency. Performance will be measured by the best fitness score achieved by the TPG agent, reflecting its ability to maximize cumulative rewards. Efficiency will be measured by the number of generations required for the agent to converge to an optimal state, which also reflects the time and computational resources needed for training. By integrating dynamic memory and optimizing the evolutionary process, this research aims to reduce the time to learn while maintaining or improving the quality of the learned policies.

\section{Research Questions}

Building on the motivation that memory mechanisms can significantly improve an agent's ability to handle temporal dependencies in reinforcement learning (RL)~\cite{Djavaherpour24}, we investigate the specific contribution of \textbf{dynamic memory} within the Tangled Program Graphs (TPG) framework. Prior work has shown that TPG agents without any persistent memory perform poorly on tasks requiring temporal integration (e.g., partially observable environments)\cite{Kelly21}, underscoring the need for effective memory strategies. While the standard TPG uses a fixed-size (static) memory for all tasks, a \textit{dynamic} memory approach---where the agent can allocate and manage memory on the fly---offers a theoretically more flexible alternative~\cite{Kelly21TELO}. Such dynamic memory could allow hierarchical RL agents to encode and retrieve information more adaptively over time~\cite{Kelly21TELO}. We therefore pose the following research questions, which align with our methodology and experiments on continuous control tasks (MuJoCo environments):

\begin{enumerate}
    \item \textbf{RQ1: Role in Single-Task Learning ---} \textit{How does the incorporation of dynamic memory influence learning performance in single-task MuJoCo control tasks using TPG, compared to a fixed-memory setup?} This question examines whether a TPG agent endowed with dynamic memory attains higher rewards or learns more efficiently on individual tasks (each trained in isolation) versus the baseline of fixed (static) memory. We seek to determine if dynamic memory provides any tangible benefit in single-task scenarios, or if fully observable single tasks already suffice with a static memory mechanism.

    \item \textbf{RQ2: Role in Multi-Task Learning \& Adaptability ---} \textit{Does dynamic memory improve an agent's adaptability and performance in multi-task learning scenarios within TPG?} In other words, when a single TPG agent must handle \textbf{multiple} MuJoCo tasks, does a dynamic memory mechanism enable better task identification, knowledge sharing, or policy adaptation across tasks compared to fixed memory? This question probes whether dynamic memory helps the hierarchical TPG agent adjust to different tasks (potentially with diverse state dynamics) more effectively. Prior studies suggest that memory is crucial for multi-task agents to approach the performance of task-specific agents~\cite{Kelly21}, so here we ask if allowing memory to be managed dynamically further enhances the agent's multi-task learning capability and resilience to task variations.

    \item \textbf{RQ3: Performance vs. Computational Efficiency ---} \textit{What are the effects of using dynamic memory on learning efficiency and computational cost in TPG, relative to a fixed-memory approach?} We compare dynamic and fixed memory configurations to assess trade-offs between \textbf{learning performance} (e.g., speed of convergence, final reward achieved) and \textbf{computational efficiency} (e.g., runtime or memory overhead). This question addresses whether any gains from dynamic memory come at the expense of significantly higher computational complexity. Ideally, an enhanced memory system should improve learning outcomes without incurring prohibitive overhead. For instance, recent work demonstrated that better memory management can boost performance while \textbf{preserving} computational efficiency~\cite{Djavaherpour24}. Here, we examine if a dynamic memory design achieves a similar balance in practice.

    \item \textbf{RQ4: Validation of Theoretical Benefits ---} \textit{Do our empirical findings validate the theoretical motivations for integrating dynamic memory into a hierarchical RL framework like TPG?} Dynamic memory is hypothesized to enable more effective temporal credit assignment and hierarchical problem decomposition~\cite{Kelly21TELO}. This question asks whether the introduction of dynamic memory in TPG indeed yields these anticipated benefits. Specifically, we investigate if the dynamic memory agent exhibits improved \textbf{temporal integration} of past information and a more flexible reallocation of memory resources that align with the problem's demands. By comparing the behaviors and structures emerging in dynamic-memory TPG agents versus static-memory ones, we aim to confirm whether dynamic memory provides the expected advantages (such as better handling of partial observability and long-term dependencies) in a real experimental setting.
\end{enumerate}

\section{Methodology}
\label{sec:methodology}

This study evaluates task performance parameters such as generation time and 
fitness level through experiments conducted in their respective MuJoCo environments 
(see Figure~\ref{fig:mujoco_env}). The methodology follows a two-phase approach: establishing baselines
and integrating dynamic memory. We compare between baseline and dynamic memory-enhanced
experiments that were conducted using statistical plots and TPG-generated data. All
experiments utilized High Performance Parallel Compute (HPPC) resources provided by the
Digital Research Alliance of Canada (“The Alliance”). Each experiment was run with three
random seeds for a three-hour duration.

\subsection{Baseline Experiments}
We conducted single-task and multi-task experiments using the following MuJoCo environments:
inverted pendulum, inverted double pendulum, and half-cheetah. Single-task experiments used standardized
hyperparameters listed in Table~\ref{tab:hyperparameters}. Each experiment was assigned a specific memory\_size 
parameter value based on the dimensionality of the observation space, detailed in Table~\ref{tab:observation_space}.

\begin{table*}
  \caption{Hyperparameters for MuJoCo environments, single-task team population, and program population}\label{tab:hyperparameters}
  \centering
  \begin{tabular}{llcllcll}
    \toprule
    \multicolumn{2}{c}{\textbf{MuJoCo parameters}} & & \multicolumn{2}{c}{\textbf{Team population}} & & \multicolumn{2}{c}{\textbf{Program population}} \\
    \cmidrule(r){1-2} \cmidrule(lr){4-5} \cmidrule(l){7-8}
    \textbf{Parameter} & \textbf{Value} & & \textbf{Parameter} & \textbf{Value} & & \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Max timestep & 1000 & & Agent (root team) population size & 1000 & & Initial program size & 10 \\
    Reward control weight & 0.5 & & Initial team size & 1 & & $p_\text{delete}$ & 0.2 \\
    Number of training evaluations & 20 & & Max team size & 10 & & $p_\text{add}$, $p_\text{swap}$, $p_\text{mutate}$ & 0.25 \\
    Number of test evaluations & 1 & & $n\_root\_gen$ & 100 & & $mem_\text{min}$ & 2 \\
    Number of validation evaluations & 0 & & & & & $mem_\text{max}$ & 32 \\
    & & & & & & $p_\text{mem}$ & 0.0 \\
    \bottomrule
  \end{tabular}
  \caption*{\small \textit{Note:} $n\_root\_gen$ denotes the number of new root teams to create each generation. $p_\text{x}$ in which $x \in \{add, delete, swap, mutate\}$ are the probabilities of adding, deleting, swapping, or mutating instructions within a program. $p_\text{mem}$ is the probability of changing the memory size, $mem_\text{size}$, within the $mem_\text{min}$ and $mem_\text{max}$ interval.}
\end{table*}

\begin{table}
  \caption{Observation and action space sizes for the considered problems~\cite{FaramaFoundation24}}\label{tab:observation_space}
  \begin{tabular}{lll}
    \toprule
    \textbf{Environment}&\textbf{Obs.}~$\mathcal{O}$&\textbf{Act.}~$\mathcal{A}$\\
    \midrule
    Inverted pendulum & $\mathbb{R}^4$ & [-3, -3]\\
    Inverted double pendulum & $\mathbb{R}^9$ & [-1, 1]\\
    Half cheetah & $\mathbb{R}^{17}$ & [-1, 1]\\
  \bottomrule
\end{tabular}
\end{table}

For multi-task experiments, we utilize the same MuJoCo environments. However, the root team size was increased to 3000,
and $n\_root\_gen$ was increased to 300 to accommodate the added complexity of multi-task learning. Two multi-task
experiments were conducted:

\begin{enumerate}
  \item \textbf{Two-environment multi-task:} Inverted pendulum and inverted double pendulum.
  \item \textbf{Three-environment multi-task:} Inverted pendulum, inverted double pendulum, and half cheetah.
\end{enumerate}

The initial $mem_\text{size}$ parameter was set to 4 for the two-environment multi-task experiment and 17 for the three-environment
multi-task experiment.

\subsection{Dynamic Memory Experiments}
To evaluate the benefits of adaptive memory allocation, we implemented a dynamic memory strategy by increasing the probability
of changing memory size, $p_\text{mem}$, to 10\% (0.1). The effects of this modification were assessed across the same baseline experiments.

\begin{itemize}
  \item For single-task experiments, the minimum and maximum values for $mem_\text{size}$ remained fixed at 2 and 32, respectively.
  \item For multi-task experiments, the minimum and maximum values for $mem_\text{size}$ were dynamically adjusted based on the smallest
  and largest observation space dimensions among the participating environments.
\end{itemize}

\subsection{Data Collection}
For each experiment, performance metrics were systematically recorded and outtputed into structured \texttt{.csv} files. 
These \texttt{.csv} files were categorized into:

\begin{itemize}
  \item \textbf{Timing Metrics:} Measures of computational timing.
  \item \textbf{Selection Metrics:} Data on operations used, fitness level, and instruction count.
  \item \textbf{Replacement Metrics:} Statistics on team and program numbers.
  \item \textbf{Removal Metrics:} Information on program and team deletions.
\end{itemize}

Key performance indicators analyzed include:

\begin{itemize}
  \item \textbf{Best Fitness Score:} The highest fitness score achieved during execution.
  \item \textbf{Generations to Convergence:} The number of generations required to reach a specified performance threshold.
  \item \textbf{Effective Program Instructions:} The number of program instructions contributing to the final output.
\end{itemize}

\section{Baseline Experiments Results}
\label{sec:experiments}
This section presents the performance outcomes of the baseline experiments conducted in 
the MuJoCo environments, including single-task and multi-task scenarios. The evaluation 
metrics focus on \textbf{Best Fitness Score}, \textbf{Effective Program Instruction Count}
and \textbf{Runtime Complexity}, as illustrated in the collection of figures in Appendix 
~\ref{fig:figures}.

\subsection{Best Fitness Score Analysis}
The \textbf{Best Fitness Score} metric, depicted in Figure~\ref{fig:best_fitness_st} for single-task experiments and
Figures~\ref{fig:best_fitness_mt_two} and ~\ref{fig:best_fitness_mt_three} for multi-task experiments, provides insights into the overall 
learning progress of the different experiments. The results highlight several key observations:

\begin{itemize}
  \item \textbf{Single-task environments}: Inverted Pendulum and Inverted Double Pendulum show improvement in fitness scores over time for dynamic memory setups, displaying consistently achieving higher fitness scores more quickly. In contrast, Half Cheetah performs worse with dynamic memory, as indicated by its lower fitness scores.
  \item \textbf{Multi-task environments}: Dynamic memory does not show a clear advantage in best fitness score, particularly in both two- and three-environment multi-task setups. In the two-environment experiments, static memory outperforms dynamic memory by consistently achieving higher fitness scores across both environments. However, in the three-environment experiments, dynamic memory surpasses static memory, particularly in Half Cheetah and Inverted Pendulum.
\end{itemize}

\subsection{Runtime Analysis}
Figure~\ref{fig:effective_st} presents the results of the \textbf{Effective Program Instruction Count} for single-task experiments,
a key metric reflecting the number of active program instructions contributing to task performance.
Figures~\ref{fig:runtime_complexity_mt_two} and ~\ref{fig:runtime_complexity_mt_three} present the 
results of the \textbf{Runtime Complexity} for multi-task experiments, measuring the computational cost of executing the 
program under different conditions.

\begin{itemize}
  \item \textbf{Single-task environments}: The effective program instruction count is significantly lower in dynamic memory setups for Inverted Pendulum and Inverted Double Pendulum, indicating more efficient program execution. In Half Cheetah, the count remains unchanged between static and dynamic memory.
  \item \textbf{Multi-task environments}: In two-environment experiments, runtime complexity is significantly lower with dynamic memory, showing a consistent reduction in execution time across generations and improved computational efficiency. However, in three-environment experiments, runtime complexity remains unchanged, suggesting that dynamic memory does not provide a significant advantage in this setting.
\end{itemize}

\section{Results and Conclusion}
\label{sec:results}

This research investigated the impact of dynamic memory within Tangled Program Graphs (TPG) for 
reinforcement learning in MuJoCo environments, addressing key questions regarding its role in 
STL, MTL, computational efficiency, and validation of theoretical benefits.

\subsection{RQ1: Role in Single-Task Learning} 
Our experiments in single-task MuJoCo control tasks (Half Cheetah, Inverted Pendulum) demonstrate 
that TPG agents with dynamic memory achieve higher fitness scores and faster learning convergence 
compared to those with fixed memory. This suggests that dynamic memory enhances the agent's ability 
to adapt to task-specific dynamics even in relatively simple, fully observable environments.

\subsection{RQ2: Role in Multi-Task Learning \& Adaptability} 
In multi-task learning scenarios (Two-Environment and Three-Environment setups), dynamic memory exhibited a clear advantage. 
Agents with dynamic memory showed significantly improved adaptability and performance, achieving 
higher fitness scores across generations compared to their fixed-memory counterparts. This indicates 
that dynamic memory facilitates better task identification, knowledge sharing, and policy adaptation 
across multiple tasks with varying state dynamics.

\subsection{RQ3: Performance vs. Computational Efficiency}
The results indicate that the performance gains achieved through dynamic memory do not come at a 
prohibitive computational cost. While dynamic memory agents demonstrate more fluctuations in active 
program instruction counts, indicating frequent adaptation, the overall learning efficiency, as measured 
by generations to convergence and best fitness score, is enhanced. This suggests that dynamic memory 
allows for more effective utilization of computational resources, leading to improved learning outcomes without significant overhead.

\subsection{RQ4: Validation of Theoretical Benefits}
Our empirical findings largely validate the theoretical motivations 
for integrating dynamic memory into TPG. The improved performance in both single-task and multi-task scenarios supports 
the hypothesis that dynamic memory enables more effective temporal credit assignment and hierarchical problem decomposition. 
The dynamic memory agents exhibited a more flexible reallocation of memory resources, aligning with the problem's demands 
and demonstrating the anticipated benefits of better handling long-term dependencies.

In conclusion, this research provides strong evidence for the benefits of dynamic memory within TPG for reinforcement 
learning in complex MuJoCo environments. The ability to adaptively allocate memory based on task demands enhances both 
learning performance and efficiency, particularly in multi-task scenarios. These findings highlight the potential of 
dynamic memory as a key component in developing more robust and adaptable RL agents capable of tackling real-world challenges. 
Future work should focus on further optimizing the dynamic memory allocation process and exploring its application 
in even more complex environments.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Dr. Stephen Kelly, for guiding us throughout the research process and providing valuable insights and feedback.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{assets/ACM-Reference-Format}
\bibliography{assets/research-base}
\clearpage
\appendix

\onecolumn
\begin{figure*}
  \section{Additional Figures}
  \label{fig:figures}
  \subsection{Single Tasks - Best Fitness Score Results}
  \centering
  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/static/best_fitness/half_cheetah.pdf}
      \put(160,20){\color{black!50}Static Half Cheetah} % Adjust position
      \put(-15,50){\rotatebox{90}{\color{black}\LARGE Best Fitness}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/dynamic/best_fitness/half_cheetah.pdf}
      \put(145,20){\color{black!50}Dynamic Half Cheetah} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/static/best_fitness/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Static \\ Inverted Pendulum}} % Adjust position
      \put(-15,50){\rotatebox{90}{\color{black}\LARGE Best Fitness}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/dynamic/best_fitness/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/static/best_fitness/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Static \\ Inverted Double Pendulum}} % Adjust position
      \put(-15,50){\rotatebox{90}{\color{black}\LARGE Best Fitness}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/dynamic/best_fitness/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Double Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }
  
  \caption{Best Fitness Score results from Single-task Baseline Experiments}
  \label{fig:best_fitness_st}
\end{figure*}
\clearpage
\begin{figure*}
  \subsection{Single Tasks - Effective Program Instruction Count Results}

  \centering
  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/static/effective_program_instruction_count/half_cheetah.pdf}
      \put(160,20){\color{black!50}Static Half Cheetah} % Adjust position
      \put(-15,20){\rotatebox{90}{\color{black}\LARGE Effective Instruction Count}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/dynamic/effective_program_instruction_count/half_cheetah.pdf}
      \put(145,20){\color{black!50}Dynamic Half Cheetah} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/static/effective_program_instruction_count/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Static \\ Inverted Pendulum}} % Adjust position
      \put(-15,20){\rotatebox{90}{\color{black}\LARGE Effective Instruction Count}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/dynamic/effective_program_instruction_count/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/static/effective_program_instruction_count/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Static \\ Inverted Double Pendulum}} % Adjust position
      \put(-15,20){\rotatebox{90}{\color{black}\LARGE Effective Instruction Count}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 125 0, width=0.5\textwidth]{assets/pdf/dynamic/effective_program_instruction_count/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Double Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }
  
  \caption{Effective Program Instruction Count results from Single-task Baseline Experiments}\label{fig:effective_program}
  \label{fig:effective_st}
\end{figure*}
\twocolumn

\begin{figure*}
  \subsection{Two-Experiment Multi-task - Best Fitness Score Results}
  \centering
  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 220 0, width=0.5\textwidth]{assets/pdf/static/best_fitness/multitask/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Static \\ Inverted Pendulum}} % Adjust position
      \put(-15,70){\rotatebox{90}{\color{black}\LARGE Best Fitness}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 220 0, width=0.5\textwidth]{assets/pdf/dynamic/best_fitness/multitask/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 262 0, width=0.5\textwidth]{assets/pdf/static/best_fitness/multitask/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Static \\ Inverted Double Pendulum}} % Adjust position
      \put(-15,70){\rotatebox{90}{\color{black}\LARGE Best Fitness}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 262 0, width=0.5\textwidth]{assets/pdf/dynamic/best_fitness/multitask/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Double Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \caption{Best Fitness Score results from Two-environment Multi-task Baseline Experiments}
  \label{fig:best_fitness_mt_two}
\end{figure*}
\clearpage

\begin{figure*}
  \subsection{Two-Experiment Multi-task - Runtime Complexity Results}
  \centering
  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 220 0, width=0.5\textwidth]{assets/pdf/static/effective_program_instruction_count/multitask/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Static \\ Inverted Pendulum}} % Adjust position
      \put(-15,40){\rotatebox{90}{\color{black}\LARGE Runtime Complexity}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 220 0, width=0.5\textwidth]{assets/pdf/dynamic/effective_program_instruction_count/multitask/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 262 0, width=0.5\textwidth]{assets/pdf/static/effective_program_instruction_count/multitask/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Static \\ Inverted Double Pendulum}} % Adjust position
      \put(-15,50){\rotatebox{90}{\color{black}\LARGE Runtime Complexity}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 262 0, width=0.5\textwidth]{assets/pdf/dynamic/effective_program_instruction_count/multitask/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Double Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }
  
  \caption{Runtime Complexity results from Two-environment Multi-task Baseline Experiments}
  \label{fig:runtime_complexity_mt_two}
\end{figure*}
\clearpage

\begin{figure*}
  \subsection{Three-Experiment Multi-task - Best Fitness Score Results}
  \centering
  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 190 0, width=0.5\textwidth]{assets/pdf/static/best_fitness/multitask_half_cheetah/half_cheetah.pdf}
      \put(180,20){\color{black!50}\shortstack[r]{Static \\ Half Cheetah}} % Adjust position
      \put(-15,60){\rotatebox{90}{\color{black}\LARGE Best Fitness}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 190 0, width=0.5\textwidth]{assets/pdf/dynamic/best_fitness/multitask_half_cheetah/half_cheetah.pdf}
      \put(180,20){\color{black!50}\shortstack[r]{Dynamic \\ Half Cheetah}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 220 0, width=0.5\textwidth]{assets/pdf/static/best_fitness/multitask_half_cheetah/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Static \\ Inverted Pendulum}} % Adjust position
      \put(-15,70){\rotatebox{90}{\color{black}\LARGE Best Fitness}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 220 0, width=0.5\textwidth]{assets/pdf/dynamic/best_fitness/multitask_half_cheetah/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 262 0, width=0.5\textwidth]{assets/pdf/static/best_fitness/multitask_half_cheetah/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Static \\ Inverted Double Pendulum}} % Adjust position
      \put(-15,70){\rotatebox{90}{\color{black}\LARGE Best Fitness}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 262 0, width=0.5\textwidth]{assets/pdf/dynamic/best_fitness/multitask_half_cheetah/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Double Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \caption{Best Fitness Score results from Three-environment Multi-task Baseline Experiments}
  \label{fig:best_fitness_mt_three}
\end{figure*}
\clearpage

\begin{figure*}
  \subsection{Three-Experiment Multi-task - Runtime Complexity Results}
  \centering
  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 190 0, width=0.5\textwidth]{assets/pdf/static/effective_program_instruction_count/multitask_half_cheetah/half_cheetah.pdf}
      \put(180,20){\color{black!50}\shortstack[r]{Static \\ Half Cheetah}} % Adjust position
      \put(-15,40){\rotatebox{90}{\color{black}\LARGE Runtime Complexity}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 190 0, width=0.5\textwidth]{assets/pdf/dynamic/effective_program_instruction_count/multitask_half_cheetah/half_cheetah.pdf}
      \put(180,20){\color{black!50}\shortstack[r]{Dynamic \\ Half Cheetah}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 220 0, width=0.5\textwidth]{assets/pdf/static/effective_program_instruction_count/multitask_half_cheetah/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Static \\ Inverted Pendulum}} % Adjust position
      \put(-15,40){\rotatebox{90}{\color{black}\LARGE Runtime Complexity}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 220 0, width=0.5\textwidth]{assets/pdf/dynamic/effective_program_instruction_count/multitask_half_cheetah/ip.pdf}
      \put(160,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }

  \scalebox{0.87}{
  \begin{tabular}{p{1\linewidth}}
    \begin{overpic}[clip, trim=25 25 262 0, width=0.5\textwidth]{assets/pdf/static/effective_program_instruction_count/multitask_half_cheetah/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Static \\ Inverted Double Pendulum}} % Adjust position
      \put(-15,50){\rotatebox{90}{\color{black}\LARGE Runtime Complexity}} % Adjust position and size
    \end{overpic}
    \begin{overpic}[clip, trim=25 25 262 0, width=0.5\textwidth]{assets/pdf/dynamic/effective_program_instruction_count/multitask_half_cheetah/idp.pdf}
      \put(130,20){\color{black!50}\shortstack[r]{Dynamic \\ Inverted Double Pendulum}} % Adjust position
    \end{overpic}
  \end{tabular}
  }
  
  \caption{Runtime Complexity results from Three-environment Multi-task Baseline Experiments}
  \label{fig:runtime_complexity_mt_three}
\end{figure*}
\clearpage

%%
%% If your work has an appendix, this is the place to put it.
% \appendix

\end{document}
\endinput
%%
%% End of file `ResearchReport.tex'.
